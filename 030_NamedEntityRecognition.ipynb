{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLZSjJxghMJC"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrEb1MBOhMJD"
   },
   "source": [
    "# 3.0 명명된 엔터티 인식기 구축\n",
    "\n",
    "이 노트북에서는 의학 질병 초록에서 질병명을 찾는 애플리케이션을 구축합니다. 이 모델은 목록에서 이름을 \"검색\"하기보다는 언어의 맥락에서 특정 단어가 질병 참조임을 \"인식\"합니다.\n",
    "\n",
    "**[3.1 커맨드 라인에서 토큰 분류](#3.1-커맨드-라인에서-토큰-분류)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 데이터 입력](#3.1.1-데이터-입력)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1.1 IOB 태깅](#3.1.1.1-IOB-태깅)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 환경 구성 파일](#3.1.2-환경-구성-파일)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3 Hydra-Enabled Python 스크립트](#3.1.3-Hydra-Enabled-Python-스크립트)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4 예제: 모델 트레이닝](#3.1.4-예제:-모델-트레이닝)<br>\n",
    "**[3.2 도메인별 트레이닝](#3.2-도메인별-트레이닝)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 TensorBoard로 결과 시각화하기](#3.2.1-TensorBoard로-결과-시각화하기)<br>\n",
    "**[3.3 평가](#3.3-평가)**<br>\n",
    "**[3.4 추론](#3.4-추론)**<br>\n",
    "\n",
    "NER 태스크의 경우, 텍스트 분류와 동일한 기본 단계에 따라 프로젝트를 구축하고 트레이닝 후 테스트 합니다. 그러나 이번에는, *도메인에 특정된*  BioMegatron 언어 모델에 있는 분류자를 트레이닝 할 예정입니다. BioMegatron은 대용량의 생의학 언어 코퍼스 ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts와 전체 텍스트 상업적 사용 컬렉션) 에서 사전 훈련된 [BERT](https://arxiv.org/abs/1810.04805)-like [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) 모델입니다.  우리는 질병 데이터 세트가 같은 생의학 영역에서 나왔기 때문에 일반 언어 모델과 비교하여 BioMegatron 에서 더 나은 성능을 기대할 수 있습니다. \n",
    "\n",
    "BioMegatron에 몇 가지 대안이 있으며 가장 눈에  띄는 모델은 [BioBERT](https://arxiv.org/abs/1901.08746)입니다. BioBERT와 비교하여 BioMegatron은 모델 사이즈에서 더 크며 더 큰 텍스트 코퍼스(말뭉치)에 대해 사전 훈련되어 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxC9kN_chMJE"
   },
   "source": [
    "---\n",
    "# 3.1 커맨드 라인에서 토큰 분류\n",
    "저희가 이번 실습에서 답변하고자 하는 질문은 다음과 같습니다.: \n",
    "\n",
    "**의학적 Abstract에서 주어진 문장에는 어떤 질병이 언급되었나요?**<br>\n",
    "\n",
    "NeMo에서 사용 가능한 NLP 모델을 상기시켜 봅니다. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RKEsnbkihMJF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnemo/examples/nlp\u001b[00m\n",
      "├── \u001b[01;34mdialogue_state_tracking\u001b[00m\n",
      "├── \u001b[01;34mentity_linking\u001b[00m\n",
      "├── \u001b[01;34mglue_benchmark\u001b[00m\n",
      "├── \u001b[01;34minformation_retrieval\u001b[00m\n",
      "├── \u001b[01;34mintent_slot_classification\u001b[00m\n",
      "├── \u001b[01;34mlanguage_modeling\u001b[00m\n",
      "├── \u001b[01;34mmachine_translation\u001b[00m\n",
      "├── \u001b[01;34mquestion_answering\u001b[00m\n",
      "├── \u001b[01;34mtext2sparql\u001b[00m\n",
      "├── \u001b[01;34mtext_classification\u001b[00m\n",
      "└── \u001b[01;34mtoken_classification\u001b[00m\n",
      "\n",
      "11 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree nemo/examples/nlp -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqfivcPIhMJF"
   },
   "source": [
    "우리는 NER을 위해 [토큰 분류](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html) 모델을 활용합니다. 왜냐하면 우리는 질병과 연관되는 단어를 분류하기 위해 \"token(토큰)\" 레벨에서 분류가 필요하기 때문입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4nCDeIihMJG"
   },
   "source": [
    "## 3.1.1 데이터 입력\n",
    "[1.0 데이터 살펴보기](010_ExploreData.ipynb) 노트북에서 확인한 것과 같이, NER 프로젝트를 위한 데이터세트는 질병 이름을 위해 IOB 태깅이 포함된 문장으로 구성딥니다. 여기서 문장의 각 단어는 명명된 엔티티의 내부(inside), 외부(outside), 시작(beginning)으로 태그가 지정됩니다. \n",
    "\n",
    "트레이닝 텍스트와 레이블 파일로는 각각 `text_train.txt`, `labels_train.txt`이 있습니다. 유효성 검증과 테스트 파일은 비슷한 이름 패턴을 따릅니다. 데이터 파일의 위치를 확인해 봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xepJDXzlhMJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0M\n",
      "-rw-r--r-- 1 702112 10513  181K Jul 13  2020 dev.tsv\n",
      "-rw-r--r-- 1 702112 10513     5 Jul 13  2020 label_ids.csv\n",
      "-rw-r--r-- 1 702112 10513    52 Jul 13  2020 label_stats.tsv\n",
      "-rw-r--r-- 1 702112 10513   48K Jul 13  2020 labels_dev.txt\n",
      "-rw-r--r-- 1 702112 10513   49K Jul 13  2020 labels_test.txt\n",
      "-rw-r--r-- 1 702112 10513  271K Jul 13  2020 labels_train.txt\n",
      "-rw-r--r-- 1 702112 10513  185K Jul 13  2020 test.tsv\n",
      "-rw-r--r-- 1 702112 10513  135K Jul 13  2020 text_dev.txt\n",
      "-rw-r--r-- 1 702112 10513  138K Jul 13  2020 text_test.txt\n",
      "-rw-r--r-- 1 702112 10513  758K Jul 13  2020 text_train.txt\n",
      "-rw-r--r-- 1 702112 10513 1023K Jul 13  2020 train.tsv\n",
      "-rw-r--r-- 1 702112 10513  1.2M Jul 13  2020 train_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "NER3_DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "!ls -lh $NER3_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4mtjt9DshMJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "text_test.txt sample\n",
      "*****\n",
      "Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \n",
      "Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \n",
      "The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \n",
      "\n",
      "*****\n",
      "labels_test.txt sample\n",
      "*****\n",
      "O O O O O O B I I O O O B I I I I O \n",
      "B I I O B I I O O O B I I I I O O O O O O O O O O O O O O O O O \n",
      "O O O B O O B I O O O O O B I I O O O O O O O O O O \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(\"*****\\ntext_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/text_test.txt\n",
    "print(\"\\n*****\\nlabels_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/labels_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VhYVTnRhMJI"
   },
   "source": [
    "### 3.1.1.1 IOB 태깅\n",
    "NER 데이터 세트의 문장과 레이블은 _inside, outside, beginning (IOB)_ 태깅을 사용하여 서로 매핑됩니다. 이 매커니즘은 일반적으로 복수의 이름으로 명명된 엔티티 유형에도 사용할 수 있습니다. :\n",
    "* B-{CHUNK_TYPE} – Beginning 청크 내에 있는 단어\n",
    "* I-{CHUNK_TYPE} – 청크 내부(Inside)에 있는 단어들 \n",
    "* O – 모든 청크의 외부(Outside)에 있음\n",
    "\n",
    "이번 실습의 경우, 우리는 엔티티 (또는 청크) 유형으로의 \"질병\"만 찾고 있으므로, 우리는 다음의 3가지 클래스 이외는 식별할 필요가 없습니다. : I, O, B<br>\n",
    "**세 개의 클래스**\n",
    "* B - 질병 이름의 시작\n",
    "* I - 질병 이름 내부에 있는 단어\n",
    "* O - 모든 질병명을 제외\n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "다음은 `labels.csv` 파일에 정의되어 있습니다.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LDnqLN0-hMJJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "B\n",
      "I"
     ]
    }
   ],
   "source": [
    "!head $NER3_DATA_DIR/label_ids.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6oCbzukhMJJ"
   },
   "source": [
    "우리가 언어 부분 분석에서 명사와 동사와 같은 2 종류의 명명된 엔티티를 찾는다면, 우리는 5 등급 IOB 체계를 사용할 것 입니다.:<br>\n",
    "**다섯개 클래스**\n",
    "* B-N - 명사 단어나 구문의 시작\n",
    "* I-N - 명사 단어나 구문 내부 \n",
    "* B-V - 동사 단어나 구문 시작\n",
    "* I-V - 동사 단어나 구문 내부\n",
    "* O   - 모든 명사나 동사를 제외\n",
    "\n",
    "해당 주제에 대해 더 자세히 알아보려면 [다음 문서](http://cs229.stanford.edu/proj2005/KrishnanGanapathy-NamedEntityRecognition.pdf)를 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkagxDMWhMJK"
   },
   "source": [
    "NCBI_ner-3 질병 데이터는 다음 [문서](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html#data-input-for-token-classification-model)의 설명에 따라 토큰 분류를 위해 올바른 유형으로 되어 있으므로 우리는 다음 환경 구성 파일을 확인해볼 준비가 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVWzvUsghMJK"
   },
   "source": [
    "## 3.1.2 환경 구성 파일\n",
    "토큰 분류 디렉터리에 대한 자세한 내용은 다음을 참조하십시오.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W2tXny7rhMJK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/dli/task/nemo/examples/nlp/token_classification\u001b[00m\n",
      "├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── punctuation_capitalization_config.yaml\n",
      "│   └── token_classification_config.yaml\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── get_tatoeba_data.py\n",
      "│   ├── import_from_iob_format.py\n",
      "│   └── prepare_data_for_punctuation_capitalization.py\n",
      "├── punctuation_capitalization_evaluate.py\n",
      "├── punctuation_capitalization_train.py\n",
      "├── token_classification_evaluate.py\n",
      "└── token_classification_train.py\n",
      "\n",
      "2 directories, 9 files\n"
     ]
    }
   ],
   "source": [
    "TC_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "!tree $TC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3_eUGd_hMJL"
   },
   "source": [
    "NER를 위한 환경 구성 파일인  `token_classification_config.yaml`은 파일 위치, 사전 훈련 모델, 하이퍼 파라미터와 같은 모델, 트레이닝, 실험 관리 세부 정보를 지정합니다. 이 패턴은 기존에 텍스트 분류 구성 파일에서 사용되는 것과 동일한 패턴입니다. 우리는 텍스트 분류 프로젝트에서 소개되었던 `OmegaConf` 도구를 사용하여 각 섹션의 세부 사항을 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dJl8Rt9bhMJM",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_ids: null\n",
      "class_labels:\n",
      "  class_labels_file: label_ids.csv\n",
      "dataset:\n",
      "  data_dir: ???\n",
      "  class_balancing: null\n",
      "  max_seq_length: 128\n",
      "  pad_label: O\n",
      "  ignore_extra_tokens: false\n",
      "  ignore_start_end: false\n",
      "  use_cache: true\n",
      "  num_workers: 2\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "train_ds:\n",
      "  text_file: text_train.txt\n",
      "  labels_file: labels_train.txt\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "validation_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "test_ds:\n",
      "  text_file: text_dev.txt\n",
      "  labels_file: labels_dev.txt\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  batch_size: 64\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "head:\n",
      "  num_fc_layers: 2\n",
      "  fc_dropout: 0.5\n",
      "  activation: relu\n",
      "  use_transformer_init: true\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 5.0e-05\n",
      "  weight_decay: 0.0\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/token_classification/conf\"\n",
    "CONFIG_FILE = \"token_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "goYrkYQihMJM",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAaB1uIDhMJN"
   },
   "source": [
    "`model`섹션에서는 모든 데이터 파일을 포함하는 `dataset.data_dir` 경로가 필요합니다. 현재 사용하고 있는 실제 파일 이름은 이미 기본 값을 만족하고 있으므로 재정의 할 필요는 없습니다. \n",
    "\n",
    "첫 번째 시도에서는, 우리는 `language_model.pretrained_model_name`를  `bert-base-cased`로 재정의함으로서 다른 실험에서 도메인 특성을 가지고 있는`biomegatron-bert-345m-cased`와 결과 값을 비교할 수 있습니다. 우리는 BioMegatron을 실행하기 위해 메모리 공간을 남겨야 하므로, 우리는 `dataset.max_seq_length`와 `batch_size` 값을 줄이도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D1Xi2yzyhMJO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 5\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 16\n",
      "accelerator: ddp\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the trainer section\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD9psU_khMJO"
   },
   "source": [
    "효율성을 위해 `amp_level`을 'O1'로 설정할 수 있습니다. 우리가 비교하고자 하는 모델들이 크고 실행하는 데 오랜 시간이 소요되기 때문에 우리는 `max_epochs` 값을 작은 값으로 재정의할 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UBa9ZJhEhMJO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_dir: null\n",
      "name: token_classification_model\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the experiment manager section\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPsSh0XvhMJP"
   },
   "source": [
    "지금은 `exp_manger` 기본 설정을 변경할 필요가 없습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piGbUbfzhMJP"
   },
   "source": [
    "## 3.1.3 Hydra-Enabled Python 스크립트\n",
    "파이썬 스크립트인 `token_classification_train.py`와 `token_evaluate.py`는 환경 구성 파일에 정의된 토큰 분류 실험을 실행하기 위해 필요한 모든 것을 캡슐화 합니다. 이 경우, 트레이닝과 평가는 별도로 수행될 것으로 예상됩니다. 텍스트 분류와 마찬가지로 두 스크립트 모두 Facebook의 [Hydra](https://hydra.cc/) 도구를 구성 관리를 위해 활용함으로서 필요에 따라 구성 파일 값을 재정의함으로서 전체 실험을 커맨드 라인에서 실행할 수 있습니다. \n",
    "\n",
    "다시 설명하면, 우리가 변경하거나 재정의해야하는 파라미터 값은 다음과 같습니다.:\n",
    "\n",
    "* `model.language_model.pretrained_model_name`: 'bert-base-cased'\n",
    "* `model.dataset.data_dir`:  /dli/task/data/NCBI_ner-3\n",
    "* `model.dataset.max_seq_length`: 64\n",
    "* `model.train_ds.batch_size`: 32\n",
    "* `model.val_ds.batch_size`:  32\n",
    "* `model.test_ds.batch_size`:  32\n",
    "* `trainer.amp_level`:  \"O1\"\n",
    "* `trainer.max_epochs`:  3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Trxqatc7hMJQ"
   },
   "source": [
    "## 3.1.4 예제: 모델 트레이닝\n",
    "\n",
    "텍스트 분류 노트북에 있었던 유사한 실험을 실행한 것과 같이 `token_classification_train.py` 트레이닝 스크립트를 실행합니다. \n",
    "\n",
    "재정의를 위한 새로운 값들은 아래 셀에 제공됩니다. 적절한 재정의와 함께 명령문을 추가하고 셀을 실행합니다. 실습 수행에 어려움이 있는 경우, [솔루션](solutions/ex3.1.4.ipynb) 을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "azFnImFdhMJQ",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2024-03-04 05:23:00 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/token_classification_model/2024-03-04_05-23-00\n",
      "[NeMo I 2024-03-04 05:23:00 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: /dli/task/data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 64\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "Downloading: 100%|█████████████████████████████| 570/570 [00:00<00:00, 1.14MB/s]\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 30.1MB/s]\n",
      "Downloading: 100%|████████████████████████████| 49.0/49.0 [00:00<00:00, 110kB/s]\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 41.2MB/s]\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : /dli/task/data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2024-03-04 05:23:00 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2024-03-04 05:23:00 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2024-03-04 05:23:00 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_utils:110] Class Weights: {0: 0.36346275404707573, 2: 7.397165440174435, 1: 8.810609011816647}\n",
      "[NeMo I 2024-03-04 05:23:00 token_classification_utils:114] Class weights saved to /dli/task/data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2024-03-04 05:23:00 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:116] Setting Max Seq length to: 64\n",
      "[NeMo I 2024-03-04 05:23:13 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-04 05:23:13 data_preprocessing:301] Min: 4 |                  Max: 178 |                  Mean: 35.938237463126846 |                  Median: 34.0\n",
      "[NeMo I 2024-03-04 05:23:13 data_preprocessing:307] 75 percentile: 45.00\n",
      "[NeMo I 2024-03-04 05:23:13 data_preprocessing:308] 99 percentile: 88.77\n",
      "[NeMo W 2024-03-04 05:23:13 token_classification_dataset:145] 319 are longer than 64\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:150] subtokens: [CLS] I ##dent ##ification of AP ##C ##2 , a ho ##mo ##logue of the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour suppress ##or . [SEP]\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_dataset:264] features saved to /dli/task/data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-03-04 05:23:13 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-03-04 05:23:14 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_dev.txt:\n",
      "[NeMo I 2024-03-04 05:23:14 data_preprocessing:135] label: 0, 22092 out of 23969 (92.17%).\n",
      "[NeMo I 2024-03-04 05:23:14 data_preprocessing:135] label: 2, 1090 out of 23969 (4.55%).\n",
      "[NeMo I 2024-03-04 05:23:14 data_preprocessing:135] label: 1, 787 out of 23969 (3.28%).\n",
      "[NeMo I 2024-03-04 05:23:14 token_classification_utils:101] Total labels: 23969. Label frequencies - {0: 22092, 2: 1090, 1: 787}\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:116] Setting Max Seq length to: 64\n",
      "[NeMo I 2024-03-04 05:23:16 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-04 05:23:16 data_preprocessing:301] Min: 4 |                  Max: 122 |                  Mean: 36.812567713976165 |                  Median: 34.0\n",
      "[NeMo I 2024-03-04 05:23:16 data_preprocessing:307] 75 percentile: 47.00\n",
      "[NeMo I 2024-03-04 05:23:16 data_preprocessing:308] 99 percentile: 83.56\n",
      "[NeMo W 2024-03-04 05:23:16 token_classification_dataset:145] 53 are longer than 64\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:150] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:264] features saved to /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-03-04 05:23:16 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo W 2024-03-04 05:23:16 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [00:05<00:00, 84.6MB/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertEncoder: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-03-04 05:23:23 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-03-04 05:23:23 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f63f0a00e20>\" \n",
      "    will be used during training (effective maximum steps = 510) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 510\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | BertEncoder          | 108 M \n",
      "1 | classifier            | TokenClassifier      | 592 K \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.613   Total estimated model params size (MB)\n",
      "[NeMo W 2024-03-04 05:23:25 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.59it/s][NeMo I 2024-03-04 05:23:26 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         90.86      58.84      71.43       1487\n",
      "    B (label_id: 1)                                          2.70      28.57       4.93         42\n",
      "    I (label_id: 2)                                          2.50       6.33       3.58         79\n",
      "    -------------------\n",
      "    micro avg                                               55.47      55.47      55.47       1608\n",
      "    macro avg                                               32.02      31.25      26.65       1608\n",
      "    weighted avg                                            84.22      55.47      66.36       1608\n",
      "    \n",
      "[NeMo W 2024-03-04 05:23:26 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0:  86%|▊| 171/199 [00:15<00:02, 11.26it/s, loss=0.0783, v_num=3-00, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  87%|▊| 173/199 [00:15<00:02, 11.31it/s, loss=0.0783, v_num=3-00, val_l\u001b[A\n",
      "Epoch 0:  89%|▉| 177/199 [00:15<00:01, 11.50it/s, loss=0.0783, v_num=3-00, val_l\u001b[A\n",
      "Epoch 0:  91%|▉| 181/199 [00:15<00:01, 11.68it/s, loss=0.0783, v_num=3-00, val_l\u001b[A\n",
      "Epoch 0:  93%|▉| 185/199 [00:15<00:01, 11.85it/s, loss=0.0783, v_num=3-00, val_l\u001b[A\n",
      "Epoch 0:  95%|▉| 189/199 [00:15<00:00, 12.03it/s, loss=0.0783, v_num=3-00, val_l\u001b[A\n",
      "Epoch 0:  97%|▉| 193/199 [00:15<00:00, 12.20it/s, loss=0.0783, v_num=3-00, val_l\u001b[A\n",
      "Epoch 0:  99%|▉| 197/199 [00:15<00:00, 12.37it/s, loss=0.0783, v_num=3-00, val_l\u001b[A[NeMo I 2024-03-04 05:23:42 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.16      99.40      99.28      21648\n",
      "    B (label_id: 1)                                         85.53      85.31      85.42        769\n",
      "    I (label_id: 2)                                         91.19      86.86      88.97       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.37      98.37      98.37      23490\n",
      "    macro avg                                               91.96      90.52      91.22      23490\n",
      "    weighted avg                                            98.35      98.37      98.35      23490\n",
      "    \n",
      "Epoch 0: 100%|█| 199/199 [00:16<00:00, 12.41it/s, loss=0.0783, v_num=3-00, val_l\n",
      "                                                                                \u001b[AEpoch 0, global step 169: val_loss reached 0.06846 (best 0.06846), saving model to \"/dli/task/nemo_experiments/token_classification_model/2024-03-04_05-23-00/checkpoints/token_classification_model--val_loss=0.07-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  85%|▊| 170/199 [00:15<00:02, 11.25it/s, loss=0.04, v_num=3-00, val_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 172/199 [00:15<00:02, 11.30it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Epoch 1:  88%|▉| 176/199 [00:15<00:02, 11.48it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Epoch 1:  90%|▉| 180/199 [00:15<00:01, 11.66it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Epoch 1:  92%|▉| 184/199 [00:15<00:01, 11.84it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Epoch 1:  94%|▉| 188/199 [00:15<00:00, 12.01it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Epoch 1:  96%|▉| 192/199 [00:15<00:00, 12.19it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 196/199 [00:15<00:00, 12.36it/s, loss=0.04, v_num=3-00, val_los\u001b[A\n",
      "Validating:  90%|███████████████████████████▊   | 26/29 [00:00<00:00, 36.68it/s]\u001b[A[NeMo I 2024-03-04 05:24:01 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.35      99.22      99.29      21648\n",
      "    B (label_id: 1)                                         82.37      90.51      86.25        769\n",
      "    I (label_id: 2)                                         92.59      88.54      90.52       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.45      98.45      98.45      23490\n",
      "    macro avg                                               91.44      92.75      92.02      23490\n",
      "    weighted avg                                            98.49      98.45      98.46      23490\n",
      "    \n",
      "Epoch 1: 100%|█| 199/199 [00:16<00:00, 12.43it/s, loss=0.04, v_num=3-00, val_los\n",
      "                                                                                \u001b[AEpoch 1, global step 339: val_loss reached 0.07233 (best 0.06846), saving model to \"/dli/task/nemo_experiments/token_classification_model/2024-03-04_05-23-00/checkpoints/token_classification_model--val_loss=0.07-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  85%|▊| 170/199 [00:15<00:02, 11.14it/s, loss=0.0238, v_num=3-00, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 172/199 [00:15<00:02, 11.20it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Epoch 2:  88%|▉| 176/199 [00:15<00:02, 11.38it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Epoch 2:  90%|▉| 180/199 [00:15<00:01, 11.56it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Epoch 2:  92%|▉| 184/199 [00:15<00:01, 11.74it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Epoch 2:  94%|▉| 188/199 [00:15<00:00, 11.91it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Epoch 2:  96%|▉| 192/199 [00:15<00:00, 12.08it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Epoch 2:  98%|▉| 196/199 [00:15<00:00, 12.25it/s, loss=0.0238, v_num=3-00, val_l\u001b[A\n",
      "Validating:  90%|███████████████████████████▊   | 26/29 [00:00<00:00, 36.99it/s]\u001b[A[NeMo I 2024-03-04 05:24:21 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.28      99.36      99.32      21648\n",
      "    B (label_id: 1)                                         85.25      88.69      86.93        769\n",
      "    I (label_id: 2)                                         92.57      88.26      90.36       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.51      98.51      98.51      23490\n",
      "    macro avg                                               92.37      92.10      92.21      23490\n",
      "    weighted avg                                            98.51      98.51      98.50      23490\n",
      "    \n",
      "Epoch 2: 100%|█| 199/199 [00:16<00:00, 12.34it/s, loss=0.0238, v_num=3-00, val_l\n",
      "                                                                                \u001b[AEpoch 2, global step 509: val_loss reached 0.07492 (best 0.06846), saving model to \"/dli/task/nemo_experiments/token_classification_model/2024-03-04_05-23-00/checkpoints/token_classification_model--val_loss=0.07-epoch=2.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 2: 100%|█| 199/199 [00:19<00:00,  9.98it/s, loss=0.0238, v_num=3-00, val_l\n",
      "[NeMo W 2024-03-04 05:24:25 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 1.14 s, sys: 519 ms, total: 1.66 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 2 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmsRDPF1hMJR"
   },
   "source": [
    "결과값은 어땠나요? 로그에 다음과 같은 항목이 포함되어 있을 것입니다.\n",
    "\n",
    "```\n",
    "    label                                                precision    recall       f1           support   \n",
    "    O (label_id: 0)                                         99.34      99.35      99.34      21648\n",
    "    B (label_id: 1)                                         85.86      89.21      87.50        769\n",
    "    I (label_id: 2)                                         91.74      89.00      90.35       1073\n",
    "    -------------------\n",
    "    micro avg                                               98.54      98.54      98.54      23490\n",
    "    macro avg                                               92.31      92.52      92.40      23490\n",
    "    weighted avg                                            98.55      98.54      98.55      23490\n",
    "    \n",
    "Epoch 2: 100%|█| 199/199 [00:15<00:00, 12.45it/s, loss=0.0251, v_num=4-43, val_l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJnaFnbhMJR"
   },
   "source": [
    "---\n",
    "# 3.2 도메인별 트레이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9RedTWzhMJR"
   },
   "source": [
    "다른 실험을 시도해 봅니다. 이번에는 `model.language_model.pretrained_model_name`를 `biomegatron-bert-345m-cased`로 재정의합니다. 이 모델은 3억 4,500만 개의 파라미터를 가진 대형 모델입니다. 따라서 실행 하는 데 시간이 오래 걸립니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZzWrHkGahMJS",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2024-03-04 05:25:08 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/token_classification_model/2024-03-04_05-25-08\n",
      "[NeMo I 2024-03-04 05:25:08 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2024-03-04 05:25:08 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: /dli/task/data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 64\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: biomegatron-bert-345m-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "[NeMo I 2024-03-04 05:25:08 megatron_utils:274] Downloading from https://api.ngc.nvidia.com/v2/models/nvidia/biomegatron345mcased/versions/0/files/vocab.txt\n",
      "Downloading: 100%|█████████████████████████████| 762/762 [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 46.4MB/s]\n",
      "Downloading: 100%|███████████████████████████| 49.0/49.0 [00:00<00:00, 91.1kB/s]\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 59.1MB/s]\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : /dli/task/data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2024-03-04 05:25:09 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2024-03-04 05:25:09 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2024-03-04 05:25:09 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:107] Class weights restored from /dli/task/data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2024-03-04 05:25:09 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2024-03-04 05:25:09 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo W 2024-03-04 05:25:09 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo I 2024-03-04 05:25:09 megatron_utils:274] Downloading from https://api.ngc.nvidia.com/v2/models/nvidia/biomegatron345mcased/versions/0/files/MegatronBERT.pt\n",
      " 44% [......................                            ] 297730048 / 665251537"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..................................................] 665251537 / 665251537using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_load ....................................... None\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... False\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  DDP_impl ........................................ local\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  faiss_use_gpu ................................... False\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_allreduce .................................. False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... True\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_interval .................................... 100\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... False\n",
      "  no_load_rng ..................................... False\n",
      "  no_save_optim ................................... False\n",
      "  no_save_rng ..................................... False\n",
      "  num_attention_heads ............................. 16\n",
      "  num_layers ...................................... 24\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... True\n",
      "  openai_gelu ..................................... False\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  report_topk_accuracies .......................... []\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scaled_masked_softmax_fusion .................... False\n",
      "  scaled_upper_triang_masked_softmax_fusion ....... False\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. BertWordPieceLowerCase\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_one_sent_docs ............................... False\n",
      "  vocab_file ...................................... /root/.cache/huggingface/nemo_nlp_tmp/c6c377d258d448da6d9259a4c9660f26/tokenizer.vocab_file\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 28996) with 60 dummy tokens (new size: 29056)\n",
      "[NeMo I 2024-03-04 05:25:13 megatron_bert:109] Megatron-lm argparse args: Namespace(DDP_impl='local', adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=True, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, consumed_train_samples=0, consumed_valid_samples=0, data_impl='infer', data_parallel_size=1, data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_duration_in_mins=None, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, fp32_residual_connection=False, global_batch_size=1, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, initial_loss_scale=4294967296, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_samples=None, lr_decay_style='linear', lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, micro_batch_size=1, min_loss_scale=1.0, min_lr=0.0, mmap_warmup=False, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=29056, params_dtype=torch.float32, pipeline_model_parallel_size=1, query_in_block_prob=0.1, rampup_batch_size=None, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensor_model_parallel_size=1, tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, train_samples=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/root/.cache/huggingface/nemo_nlp_tmp/c6c377d258d448da6d9259a4c9660f26/tokenizer.vocab_file', weight_decay=0.01, world_size=1)\n",
      "[NeMo W 2024-03-04 05:25:16 megatron_bert:185] Megatron-lm checkpoint version not found. Setting checkpoint_version to 0.\n",
      "[NeMo I 2024-03-04 05:25:16 megatron_bert:192] Checkpoint loaded from from /root/.cache/torch/megatron/biomegatron-bert-345m-cased\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-03-04 05:25:16 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-03-04 05:25:16 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fcaf80508e0>\" \n",
      "    will be used during training (effective maximum steps = 510) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 510\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | MegatronBertEncoder  | 332 M \n",
      "1 | classifier            | TokenClassifier      | 1.1 M \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "333 M     Trainable params\n",
      "0         Non-trainable params\n",
      "333 M     Total params\n",
      "1,334.575 Total estimated model params size (MB)\n",
      "[NeMo W 2024-03-04 05:25:17 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:   0%|                            | 0/2 [00:00<?, ?it/s]torch distributed is already initialized, skipping initialization ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "INFO:root:Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "INFO:root:Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:01<00:01,  1.03s/it][NeMo I 2024-03-04 05:25:18 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         93.53       8.74      15.99       1487\n",
      "    B (label_id: 1)                                          1.21       7.14       2.08         42\n",
      "    I (label_id: 2)                                          5.65      87.34      10.61         79\n",
      "    -------------------\n",
      "    micro avg                                               12.56      12.56      12.56       1608\n",
      "    macro avg                                               33.46      34.41       9.56       1608\n",
      "    weighted avg                                            86.80      12.56      15.36       1608\n",
      "    \n",
      "[NeMo W 2024-03-04 05:25:18 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0:   0%|                                          | 0/199 [00:00<?, ?it/s][W reducer.cpp:1060] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  85%|▊| 170/199 [00:46<00:07,  3.66it/s, loss=0.0744, v_num=5-08, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|▊| 172/199 [00:46<00:07,  3.69it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  87%|▊| 174/199 [00:46<00:06,  3.72it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  88%|▉| 176/199 [00:46<00:06,  3.75it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  89%|▉| 178/199 [00:47<00:05,  3.79it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  90%|▉| 180/199 [00:47<00:04,  3.82it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  91%|▉| 182/199 [00:47<00:04,  3.85it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  92%|▉| 184/199 [00:47<00:03,  3.88it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  93%|▉| 186/199 [00:47<00:03,  3.91it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  94%|▉| 188/199 [00:47<00:02,  3.94it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  95%|▉| 190/199 [00:47<00:02,  3.97it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  96%|▉| 192/199 [00:47<00:01,  4.01it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  97%|▉| 194/199 [00:48<00:01,  4.04it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  98%|▉| 196/199 [00:48<00:00,  4.07it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Epoch 0:  99%|▉| 198/199 [00:48<00:00,  4.10it/s, loss=0.0744, v_num=5-08, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:01<00:00, 15.46it/s]\u001b[A[NeMo I 2024-03-04 05:26:07 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.25      99.47      99.36      21648\n",
      "    B (label_id: 1)                                         87.91      87.91      87.91        769\n",
      "    I (label_id: 2)                                         92.88      88.82      90.81       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.60      98.60      98.60      23490\n",
      "    macro avg                                               93.35      92.06      92.69      23490\n",
      "    weighted avg                                            98.59      98.60      98.60      23490\n",
      "    \n",
      "Epoch 0: 100%|█| 199/199 [00:48<00:00,  4.10it/s, loss=0.0744, v_num=5-08, val_l\n",
      "                                                                                \u001b[AEpoch 0, global step 169: val_loss reached 0.06154 (best 0.06154), saving model to \"/dli/task/nemo_experiments/token_classification_model/2024-03-04_05-25-08/checkpoints/token_classification_model--val_loss=0.06-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  85%|▊| 170/199 [00:45<00:07,  3.72it/s, loss=0.03, v_num=5-08, val_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 172/199 [00:45<00:07,  3.75it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  87%|▊| 174/199 [00:45<00:06,  3.79it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  88%|▉| 176/199 [00:46<00:06,  3.82it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  89%|▉| 178/199 [00:46<00:05,  3.85it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  90%|▉| 180/199 [00:46<00:04,  3.88it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  91%|▉| 182/199 [00:46<00:04,  3.92it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  92%|▉| 184/199 [00:46<00:03,  3.95it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  93%|▉| 186/199 [00:46<00:03,  3.98it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  94%|▉| 188/199 [00:46<00:02,  4.01it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  95%|▉| 190/199 [00:47<00:02,  4.04it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  96%|▉| 192/199 [00:47<00:01,  4.07it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  97%|▉| 194/199 [00:47<00:01,  4.10it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 196/199 [00:47<00:00,  4.14it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 198/199 [00:47<00:00,  4.17it/s, loss=0.03, v_num=5-08, val_los\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:01<00:00, 15.39it/s]\u001b[A[NeMo I 2024-03-04 05:27:05 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.31      99.49      99.40      21648\n",
      "    B (label_id: 1)                                         87.19      90.25      88.69        769\n",
      "    I (label_id: 2)                                         93.94      88.16      90.96       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.67      98.67      98.67      23490\n",
      "    macro avg                                               93.48      92.63      93.02      23490\n",
      "    weighted avg                                            98.67      98.67      98.67      23490\n",
      "    \n",
      "Epoch 1: 100%|█| 199/199 [00:47<00:00,  4.17it/s, loss=0.03, v_num=5-08, val_los\n",
      "                                                                                \u001b[AEpoch 1, global step 339: val_loss reached 0.06212 (best 0.06154), saving model to \"/dli/task/nemo_experiments/token_classification_model/2024-03-04_05-25-08/checkpoints/token_classification_model--val_loss=0.06-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  85%|▊| 170/199 [00:45<00:07,  3.73it/s, loss=0.0153, v_num=5-08, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 172/199 [00:45<00:07,  3.76it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  87%|▊| 174/199 [00:45<00:06,  3.79it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  88%|▉| 176/199 [00:46<00:06,  3.82it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  89%|▉| 178/199 [00:46<00:05,  3.86it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  90%|▉| 180/199 [00:46<00:04,  3.89it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  91%|▉| 182/199 [00:46<00:04,  3.92it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  92%|▉| 184/199 [00:46<00:03,  3.95it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  93%|▉| 186/199 [00:46<00:03,  3.98it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  94%|▉| 188/199 [00:46<00:02,  4.01it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  95%|▉| 190/199 [00:46<00:02,  4.05it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  96%|▉| 192/199 [00:47<00:01,  4.08it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  97%|▉| 194/199 [00:47<00:01,  4.11it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  98%|▉| 196/199 [00:47<00:00,  4.14it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Epoch 2:  99%|▉| 198/199 [00:47<00:00,  4.17it/s, loss=0.0153, v_num=5-08, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:01<00:00, 15.31it/s]\u001b[A[NeMo I 2024-03-04 05:28:04 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.36      99.48      99.42      21648\n",
      "    B (label_id: 1)                                         86.00      90.25      88.07        769\n",
      "    I (label_id: 2)                                         94.05      88.35      91.11       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.67      98.67      98.67      23490\n",
      "    macro avg                                               93.13      92.69      92.87      23490\n",
      "    weighted avg                                            98.68      98.67      98.67      23490\n",
      "    \n",
      "Epoch 2: 100%|█| 199/199 [00:47<00:00,  4.18it/s, loss=0.0153, v_num=5-08, val_l\n",
      "                                                                                \u001b[AEpoch 2, global step 509: val_loss reached 0.07148 (best 0.06154), saving model to \"/dli/task/nemo_experiments/token_classification_model/2024-03-04_05-25-08/checkpoints/token_classification_model--val_loss=0.07-epoch=2.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 2: 100%|█| 199/199 [00:59<00:00,  3.35it/s, loss=0.0153, v_num=5-08, val_l\n",
      "[NeMo W 2024-03-04 05:28:16 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 3.4 s, sys: 2.65 s, total: 6.06 s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 5-6 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'biomegatron-bert-345m-cased'\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOWHqEYYhMJS"
   },
   "source": [
    "## 3.2.1 TensorBoard로 결과 시각화하기\n",
    "[experiment manager](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html?highlight=tensorboard#experiment-manager) 는 텐서보드로 볼 수 있는 결과들을 저장합니다. <br>\n",
    "인스턴스에 대한 [TensorBoard](/tensorboard/)를 열어 살펴보십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaRAwDAshMJT"
   },
   "source": [
    "실행한 모델들의 결과를 비교하려면 \"f1\" 스케일러를 선택합니다. 여러분은 함께 실행한 모든 모델을 보실 수도 있고, 개별 모델을 선택하여 비교할 수 있습니다. 이번 예제 비교에서는 5개의 Epoch가 실행되었습니다. 주황색 실선은 `bert-base-cased` 모델의 결과를 보여주고 파란색 실선은 `biomegatron-bert-345m-cased` 모델의 결과를 보여줍니다. BioMegatron 모델은 질병 이름을 더 잘 식별할 수 있기 때문에 매우 빠르게 잘 동작합니다. 5개 epoch 이후에 여전히 약간 높은 f1을 가지고 있습니다. 여러분의 프로젝트를 위해 선택하는 모델은 메모리, 시간, 성능 요구사항의 제약 조건에 따라 달라질 수 있습니다. 여러분의 결과가 학습 알고리즘의 임의성으로 예시와 다를 수 있음을 안내드립니다. \n",
    "\n",
    "<img src=\"images/tensorboard_02.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCGkhEuZhMJT"
   },
   "source": [
    "---\n",
    "# 3.3 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TiQLNFjbhMJT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOuvbtJ0hMJU"
   },
   "source": [
    "테스트 세트를 통해 모델을 평가하려면 `.nemo` 훈련된 모델의 위치를 지정해야 합니다. 각 실험은 `nemo_experiments` 아래의 시간 표기한 디렉토리에 결과를 실행 합니다. 드릴 다운하면 최종 `token_classification_model.nemo` 가 있는 `checkpoints` 폴더를 찾을 수 있습니다. 다음 셀에서는 Python 로직의 일부를 사용하여 모델 목록을 캡쳐하고 최신 모델 목록을 식별해 봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IRbebUS4hMJU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest model is \n",
      "nemo_experiments/token_classification_model/2024-03-04_05-25-08/checkpoints/token_classification_model.nemo\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "nemo_model_paths = glob.glob('nemo_experiments/token_classification_model/*/checkpoints/*.nemo')\n",
    "\n",
    "# Sort newest first\n",
    "nemo_model_paths.sort(reverse=True)\n",
    "print(\"The latest model is \\n{}\".format(nemo_model_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGZdkiuchMJU"
   },
   "source": [
    "테스트 세트에 대해 평가를 진행하는 몇 가지 방법이 있습니다.:\n",
    "1. `token_classification_evaluate.py`를 기존과 같은 재정의 값과 함께 실행하고 `pretrained_model`를 `.nemo` 형식 값으로 추가 재정의 합니다. \n",
    "\n",
    "```text\n",
    "   !python $TOKEN_DIR/token_classification_evaluate.py \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        pretrained_model=$LATEST_MODEL\n",
    "```\n",
    "        \n",
    "2. 트레이닝된 모델 체크포인트를 복원하여 모델을 인스턴스화 하고 NeMo 메소드를 실행하여 테스트 세트를 평가합니다.<br>\n",
    "   우리는 이 방법으로 실습을 진행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E-C_JVGuhMJV",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2024-03-04 05:31:05 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    batch_size: 32\n",
      "    \n",
      "[NeMo W 2024-03-04 05:31:05 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 32\n",
      "    \n",
      "[NeMo W 2024-03-04 05:31:05 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 32\n",
      "    \n",
      "[NeMo W 2024-03-04 05:31:05 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2024-03-04 05:31:05 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2024-03-04 05:31:05 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_load ....................................... None\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... False\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  DDP_impl ........................................ local\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  faiss_use_gpu ................................... False\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_allreduce .................................. False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... True\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_interval .................................... 100\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... False\n",
      "  no_load_rng ..................................... False\n",
      "  no_save_optim ................................... False\n",
      "  no_save_rng ..................................... False\n",
      "  num_attention_heads ............................. 16\n",
      "  num_layers ...................................... 24\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... True\n",
      "  openai_gelu ..................................... False\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  report_topk_accuracies .......................... []\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scaled_masked_softmax_fusion .................... False\n",
      "  scaled_upper_triang_masked_softmax_fusion ....... False\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... None\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. BertWordPieceLowerCase\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_cpu_initialization .......................... False\n",
      "  use_one_sent_docs ............................... False\n",
      "  vocab_file ...................................... /tmp/tmpa5kb8huh/0cfe72d382134ca1a30c462d763885a9_tokenizer.vocab_file\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building BertWordPieceLowerCase tokenizer ...\n",
      " > padded vocab (size: 28996) with 60 dummy tokens (new size: 29056)\n",
      "[NeMo I 2024-03-04 05:31:05 megatron_bert:109] Megatron-lm argparse args: Namespace(DDP_impl='local', adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, adlr_autoresume=False, adlr_autoresume_interval=1000, apply_query_key_layer_scaling=True, apply_residual_connection_post_layernorm=False, attention_dropout=0.1, attention_softmax_in_fp32=False, bert_load=None, bias_dropout_fusion=False, bias_gelu_fusion=False, block_data_path=None, checkpoint_activations=False, checkpoint_num_layers=1, clip_grad=1.0, consumed_train_samples=0, consumed_valid_samples=0, data_impl='infer', data_parallel_size=1, data_path=None, distribute_checkpointed_activations=False, distributed_backend='nccl', eod_mask_loss=False, eval_interval=1000, eval_iters=100, exit_duration_in_mins=None, exit_interval=None, faiss_use_gpu=False, finetune=False, fp16=False, fp16_lm_cross_entropy=False, fp32_allreduce=False, fp32_residual_connection=False, global_batch_size=1, hidden_dropout=0.1, hidden_size=1024, hysteresis=2, ict_head_size=None, ict_load=None, indexer_batch_size=128, indexer_log_interval=1000, init_method_std=0.02, initial_loss_scale=4294967296, layernorm_epsilon=1e-05, lazy_mpu_init=True, load=None, local_rank=None, log_interval=100, loss_scale=None, loss_scale_window=1000, lr=None, lr_decay_iters=None, lr_decay_samples=None, lr_decay_style='linear', lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, make_vocab_size_divisible_by=128, mask_prob=0.15, max_position_embeddings=512, merge_file=None, micro_batch_size=1, min_loss_scale=1.0, min_lr=0.0, mmap_warmup=False, no_load_optim=False, no_load_rng=False, no_save_optim=False, no_save_rng=False, num_attention_heads=16, num_layers=24, num_workers=2, onnx_safe=True, openai_gelu=False, override_lr_scheduler=False, padded_vocab_size=29056, params_dtype=torch.float32, pipeline_model_parallel_size=1, query_in_block_prob=0.1, rampup_batch_size=None, rank=0, report_topk_accuracies=[], reset_attention_mask=False, reset_position_ids=False, save=None, save_interval=None, scaled_masked_softmax_fusion=False, scaled_upper_triang_masked_softmax_fusion=False, seed=1234, seq_length=None, short_seq_prob=0.1, split='969, 30, 1', tensor_model_parallel_size=1, tensorboard_dir=None, titles_data_path=None, tokenizer_type='BertWordPieceLowerCase', train_iters=None, train_samples=None, use_checkpoint_lr_scheduler=False, use_cpu_initialization=True, use_one_sent_docs=False, vocab_file='/tmp/tmpa5kb8huh/0cfe72d382134ca1a30c462d763885a9_tokenizer.vocab_file', weight_decay=0.01, world_size=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-04 05:31:08 megatron_bert:185] Megatron-lm checkpoint version not found. Setting checkpoint_version to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-04 05:31:08 megatron_bert:192] Checkpoint loaded from from /root/.cache/torch/megatron/biomegatron-bert-345m-cased\n",
      "[NeMo I 2024-03-04 05:31:09 modelPT:434] Model TokenClassificationModel was successfully restored from nemo_experiments/token_classification_model/2024-03-04_05-25-08/checkpoints/token_classification_model.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model by restoring from the .nemo checkpoint\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "LATEST_MODEL = nemo_model_paths[0]\n",
    "model = nemo_nlp.models.TokenClassificationModel.restore_from(LATEST_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT8El7_9hMJV"
   },
   "source": [
    "`evaluate_from_file` 메소드를 사용하여 테스트 세트로 모델을 평가합니다.`add_confusion_matrix`를 True로 설정하여 모델이 얼마다 잘 했는지 시각적으로 표현합니다. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oqKpdckMhMJV",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:116] Setting Max Seq length to: 157\n",
      "[NeMo I 2024-03-04 05:31:31 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-04 05:31:31 data_preprocessing:301] Min: 4 |                  Max: 157 |                  Mean: 37.204255319148935 |                  Median: 35.0\n",
      "[NeMo I 2024-03-04 05:31:31 data_preprocessing:307] 75 percentile: 46.00\n",
      "[NeMo I 2024-03-04 05:31:31 data_preprocessing:308] 99 percentile: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-04 05:31:31 token_classification_dataset:145] 0 are longer than 157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:150] subtokens: [CLS] C ##luster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic T - cell le ##uka ##emia . [SEP]\n",
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:31:31 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "[NeMo I 2024-03-04 05:31:51 token_classification_model:482] Labels save to /dli/task/nemo_experiments/token_classification_model/logs/infer_text_test.txt\n",
      "[NeMo I 2024-03-04 05:31:51 token_classification_model:488] Predictions saved to /dli/task/nemo_experiments/token_classification_model/logs/infer_text_test.txt\n",
      "[NeMo I 2024-03-04 05:31:52 utils_funcs:94] Confusion matrix saved to /dli/task/nemo_experiments/token_classification_model/logs/Normalized_Confusion_matrix_20240304-053152\n",
      "[NeMo I 2024-03-04 05:31:52 token_classification_model:499]                  precision    recall  f1-score   support\n",
      "    \n",
      "    O (label id: 0)     0.9959    0.9916    0.9938     22450\n",
      "    B (label id: 1)     0.8888    0.9240    0.9060       960\n",
      "    I (label id: 2)     0.8901    0.9384    0.9136      1087\n",
      "    \n",
      "           accuracy                         0.9866     24497\n",
      "          macro avg     0.9249    0.9513    0.9378     24497\n",
      "       weighted avg     0.9870    0.9866    0.9868     24497\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "OUTPUT_DIR = '/dli/task/nemo_experiments/token_classification_model/logs'\n",
    "model.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'text_test.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'labels_test.txt'),\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp7RoOWOhMJW"
   },
   "source": [
    "결과 값은 다음과 같이 표시 됩니다.\n",
    "\n",
    "```\n",
    "[NeMo I 2021-06-29 00:42:16 token_classification_model:499]                  precision    recall  f1-score   support\n",
    "    \n",
    "    O (label id: 0)     0.9958    0.9910    0.9934     22450\n",
    "    B (label id: 1)     0.8886    0.9135    0.9009       960\n",
    "    I (label id: 2)     0.8724    0.9374    0.9038      1087\n",
    "    \n",
    "           accuracy                         0.9856     24497\n",
    "          macro avg     0.9189    0.9473    0.9327     24497\n",
    "       weighted avg     0.9861    0.9856    0.9858     24497\n",
    "\n",
    "\n",
    "최종 Confusion 매트릭트 시각화는 밝은 대각선을 보여주며, 이는 예측 레이블이 모든 레이블 유형 (IOB)에 대해 높은 정확도로 실제 레이블과 일치했음을 나타냅니다. \n",
    "```\n",
    "\n",
    "<img src=\"images/ner_confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-2IrIw0hMJW"
   },
   "source": [
    "---\n",
    "# 3.4 추론\n",
    "쿼리 목록에 대한 추론을 실행하려면, `add_predictions` 메소드와 함께 이미 로드된 동일한 모델을 사용하십시오 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2gFkihJThMJW"
   },
   "outputs": [],
   "source": [
    "queries = [\"Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \",\n",
    "    \"Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \",\n",
    "    \"The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \",\n",
    "    \"By analysing tumour DNA from patients with sporadic T - cell prolymphocytic leukaemia ( T - PLL ) , a rare clonal malignancy with similarities to a mature T - cell leukaemia seen in A - T , we demonstrate a high frequency of ATM mutations in T - PLL . \",\n",
    "    \"In marked contrast to the ATM mutation pattern in A - T , the most frequent nucleotide changes in this leukaemia were missense mutations . \",\n",
    "    \"These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM - related proteins in mouse , yeast and Drosophila . \",\n",
    "    \"The resulting amino - acid substitutions are predicted to interfere with ATP binding or substrate recognition . \",\n",
    "    \"Two of seventeen mutated T - PLL samples had a previously reported A - T allele . \",\n",
    "    \"In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . \",\n",
    "    \"Occasional missense mutations in ATM were also found in tumour DNA from patients with B - cell non - Hodgkins lymphomas ( B - NHL ) and a B - NHL cell line . \"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wmwSH3EwhMJW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:116] Setting Max Seq length to: 74\n",
      "[NeMo I 2024-03-04 05:31:57 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-04 05:31:57 data_preprocessing:301] Min: 20 |                  Max: 74 |                  Mean: 38.1 |                  Median: 34.0\n",
      "[NeMo I 2024-03-04 05:31:57 data_preprocessing:307] 75 percentile: 44.25\n",
      "[NeMo I 2024-03-04 05:31:57 data_preprocessing:308] 99 percentile: 71.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-04 05:31:57 token_classification_dataset:145] 0 are longer than 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:150] subtokens: [CLS] C ##luster ##ing of miss ##ense mutations in the at ##ax ##ia - te ##lang ##ie ##ct ##asi ##a gene in a s ##poradic T - cell le ##uka ##emia . [SEP]\n",
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:31:57 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-04 05:31:57 token_classification_model:436] Predictions saved to predictions.txt\n"
     ]
    }
   ],
   "source": [
    "results = model.add_predictions(queries, output_file='predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gP8EG4t4hMJX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering of missense mutations in the ataxia[B] [I]- telangiectasia[I] gene in a sporadic T[B] [I]- cell[I] leukaemia[I] .\n",
      "Ataxia[B] [I]- telangiectasia[I] ( A[B] [I]- T[I] ) is a recessive[B] multi[I] [I]- system[I] disorder[I] caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) .\n",
      "The risk of cancer[B] , especially lymphoid[B] neoplasias[I] , is substantially elevated in A[B] [I]- T[I] patients and has long been associated with chromosomal instability .\n",
      "By analysing tumour[B] DNA from patients with sporadic[B] T[I] [I]- cell[I] prolymphocytic[I] leukaemia[I] ( T[B] [I]- PLL[I] ) , a rare clonal[B] malignancy[B] with similarities to a mature[B] T[I] [I]- cell[I] leukaemia[I] seen in A[B] [I]- T[I] , we demonstrate a high frequency of ATM mutations in T[B] [I]- PLL[I] .\n",
      "In marked contrast to the ATM mutation pattern in A[B] [I]- T[I] , the most frequent nucleotide changes in this leukaemia[B] were missense mutations .\n",
      "These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM - related proteins in mouse , yeast and Drosophila .\n",
      "The resulting amino - acid substitutions are predicted to interfere with ATP binding or substrate recognition .\n",
      "Two of seventeen mutated T - PLL samples had a previously reported A - T allele .\n",
      "In contrast , no mutations were detected in the p53 gene , suggesting that this tumour[B] suppressor is not frequently altered in this leukaemia[B] .\n",
      "Occasional missense mutations in ATM were also found in tumour[B] DNA from patients with B[B] [I]- cell[I] non[I] [I]- Hodgkins[I] lymphomas[I] ( B[B] [I]- NHL[I] ) and a B[B] [I]- NHL[I] cell line .\n"
     ]
    }
   ],
   "source": [
    "!cat predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tw-4VFwKhMJX"
   },
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">축하합니다!</h2>\n",
    "\n",
    "여러분은 NeMo를 마스터하고 다음을 배웠습니다:\n",
    "* 명명된 엔티티 인식기를 구축하는 방법 \n",
    "* 도메인별 모델에 적용하는 방법\n",
    "* 체크포인트에서 쿼리를 사용하여 NER 모델을 테스트하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsJFhwZZhMJX"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "030_NamedEntityRecognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
